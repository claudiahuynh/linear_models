---
title: "Bootstrapping"
output: github_document
---

## Boostrapping
Repeated sampling - repeatedly draw random samples of the same size from a population 
- For each sample, compute the mean
- The distribution of the sample mean converges to a normal distribution 

Boostraping: 
Idea is to mimic repeated sampling with the one sample you have
- Your sample is draw at random from your population
- You'd like to draw more samples but you can't
- So you draw a bootstrap sample from the one sample you have 
- The bootstrap has the same size as the original sample and is drawn with replacement
- Analyze this sample using whatever approach you want to apply
- Repeat 
Keep track of the bootstrap samples, analyses and results in a single data frame organizes the process and prevents mistakes 

```{r}
library(tidyverse)
library(modelr)
library(p8105.datasets)
set.seed(1)
```

Do some bootstrapping

Make up some data - follows linear regression 
```{r}
n_samp = 250

sim_df_constant =
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )
```

```{r}
sim_df_nonconstant = 
  sim_df_constant |> 
  mutate(
    error = error * 0.75 * x,
    y = 2 + 3 * x + error
  )
```

Let's look at these. Constant variance of residuals. Residuals are spread evenly. 

```{r}
sim_df_constant |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "lm")
```

For this one, as x approaches 0, residuals are tightly packed togher. As x increases, there's more spread in the data. This violates the assumption for LR that error term does not depend on x. CI has wider band for larger values of x

```{r}
sim_df_nonconstant |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "lm")
```


Look at regression results
```{r}
sim_df_constant |> 
  lm(y ~ x, data = _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

```{r}
sim_df_nonconstant |> 
  lm(y ~ x, data = _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

Intuitively, the two should not be so similar. But the p-values are the same.

## Draw a bootstrap sample

```{r}
boot_sample = function(df) {
  
  boot_df = 
    sample_frac(df, replace = TRUE) |>  #sampling with replacement
    arrange(x)
  
  return(boot_df)
}
```

Let's try running this. There are a couple of replacements.Therefore, different datasets everytime we run this. 

```{r}
sim_df_nonconstant |> 
  boot_sample() |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .5) +
  stat_smooth(method = "lm")
```

Grey points (non-repeated data). Black points (repeated data).

Each bootstrap sample will give a slightly different regression fit. If we run this over and over, the bootstrap sample should reflect the overall properties of the dataset. The variability across the regression lines is what we care about.


Can we do this as part of an analysis?
```{r}
sim_df_nonconstant |> 
  boot_sample() |> 
  lm(y ~ x, data = _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

## Bootstrap a lot

```{r}
boot_straps = 
  tibble(
    strap_number = 1:1000
  ) |> 
  mutate(
    strap_sample = map(strap_number, \(i)boot_sample(df = sim_df_nonconstant)),
    models = map(strap_sample, \(df)lm(y ~ x, data = df)),
    result = map(models, broom::tidy)
  )

bootstrap_results = 
  boot_straps |> 
  select(strap_number, result) |> 
  unnest(result) |> 
  group_by(term) |> 
  summarize(
    boot_se = sd(estimate)
  ) |> # different from the one that came out of SLR
  knitr::kable()
```

Bootstrap works when the assumptions aren't violated, but it will also work when assumptions are violated. 

## Do this using modelr
Modelr creates a bootstrap column directly. 
```{r}
boot_straps = 
  sim_df_nonconstant |> 
  modelr::bootstrap(1000) |> 
  mutate(
    strap = map(strap, as_tibble),
    models = map(strap, \(df) lm (y ~ x, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(.id, results) |> 
  unnest(results)
```

## What do we want to report?

```{r}
boot_straps |> 
  group_by(term) |> 
  summarize(
    boot_est = mean(estimate),
    boot_se = sd(estimate),
    boot_ci_ul = quantile(estimate, .975),
    boot_ci_ll = quantile(estimate, .025)
  )
```

## Airbnb

```{r}
data("nyc_airbnb")

manhattan_df =
  nyc_airbnb |> 
  mutate(
    stars = review_scores_location / 2 
  ) |> 
  rename(
    borough = neighbourhood_group, 
    neighborhood = neighbourhood
    ) |> 
  filter(borough == "Manhattan") |> 
  select(price, stars, room_type) |> 
  drop_na()

```

Plot the data 

```{r}
manhattan_plot = 
manhattan_df |> 
  ggplot(aes(x = stars, y = price)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)

print(manhattan_plot)
```

Fit regression 

```{r}
manhattan_df |> 
  lm(price ~ stars + room_type, data = _) |> 
  broom::tidy() |> 
  knitr::kable()
```

In this case, we don't really trust the std.error, based on the scatterplot we made.

Run the bootstrap to check to see if everything's supposed to be what it is supposed to. 

```{r}
boot_results = 
  manhattan_df |> 
  modelr::bootstrap(1000) |> 
  mutate(
    strap = map(strap, as_tibble),
    models = map(strap, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(.id, results) |> 
  unnest(results) 

boot_results |> 
  filter(term == "stars") |> 
  ggplot(aes(estimate)) +
  geom_density()
```

```{r}
boot_results |> 
  group_by(term) |> 
  summarize(
    boot_est = mean(estimate),
    boot_se = sd(estimate),
    boot_ci_ul = quantile(estimate, .975),
    boot_ci_ll = quantile(estimate, .025)
  )
```

